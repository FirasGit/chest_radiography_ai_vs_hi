{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip all files and save to a specific directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_with_zip_files = \"/data/chest_radiograph/orthanc_db_downloaded\"\n",
    "target_folder = \"/data/chest_radiograph/dicom_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_files = os.listdir(folder_with_zip_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for zip_file in tqdm(zip_files):\n",
    "    with zipfile.ZipFile(os.path.join(folder_with_zip_files, zip_file), 'r') as zip_ref:\n",
    "        zip_ref.extractall(target_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unzipping created a weirdly-named parent directory. Move all content inside that directory out of that directory and change remove the empty directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_directories = os.listdir(target_folder)\n",
    "for parent_directory in tqdm(parent_directories):\n",
    "    # Move all subfolders\n",
    "    for subfolder in os.listdir(os.path.join(target_folder, parent_directory)):\n",
    "        subfolder_absolute_path = os.path.join(target_folder, parent_directory, subfolder)\n",
    "        shutil.move(subfolder_absolute_path, target_folder)\n",
    "    shutil.rmtree(os.path.join(target_folder, parent_directory), ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the Dicom Files to Nifti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nifti_root_directory = '/data/chest_radiograph/nifti_files'\n",
    "dicom_root_directory = '/data/chest_radiograph/dicom_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_image_data(dataset):\n",
    "    sitk_img = sitk.GetImageFromArray(dataset.pixel_array)\n",
    "    sitk_img.SetSpacing(dataset[0x0018, 0x1164].value)\n",
    "    return sitk_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_meta_data_safely(sitk_img, dataset, key, value):\n",
    "    org_sitk_img = sitk_img\n",
    "    try: \n",
    "        sitk_img.SetMetaData(key, str(dataset[value[0], value[1]].value))\n",
    "        return sitk_img\n",
    "    except KeyError as e:\n",
    "        # Ignore this metadata information\n",
    "        print(e)\n",
    "        return org_sitk_img\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_meta_data(sitk_img, dataset):\n",
    "    sitk_img = set_meta_data_safely(sitk_img, dataset, \"StudyDate\", [0x0008, 0x0020])\n",
    "    sitk_img = set_meta_data_safely(sitk_img, dataset, \"StudyTime\", [0x0008, 0x0030])\n",
    "    sitk_img = set_meta_data_safely(sitk_img, dataset, \"AccessionNumber\", [0x0008, 0x0050])\n",
    "    sitk_img = set_meta_data_safely(sitk_img, dataset, \"PatientBirthdate\", [0x0010, 0x0030])\n",
    "    sitk_img = set_meta_data_safely(sitk_img, dataset, \"PatientSex\", [0x0010, 0x0040])\n",
    "    sitk_img = set_meta_data_safely(sitk_img, dataset, \"RequestingPhysician\", [0x0032, 0x1032])\n",
    "    sitk_img = set_meta_data_safely(sitk_img, dataset, \"ExposureinuAs\", [0x0018, 0x1153])\n",
    "    return sitk_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_folders = os.listdir(dicom_root_directory)\n",
    "for dicom_folder in tqdm(dicom_folders):\n",
    "    for root, dirs, files in os.walk(os.path.join(dicom_root_directory, dicom_folder)):\n",
    "        for file in files:\n",
    "            if file.endswith(\".dcm\"):\n",
    "                dicom_file_name = os.path.join(root, file)\n",
    "                dataset = pydicom.dcmread(dicom_file_name)\n",
    "                if dataset.Modality == 'CR':\n",
    "                    try:\n",
    "                        # Only take images that contain spacing and image data\n",
    "                        sitk_img = set_image_data(dataset)\n",
    "                    except Exception as e:\n",
    "                        if not isinstance(e, KeyError):\n",
    "                            print(e)\n",
    "                        continue\n",
    "                    sitk_img = set_meta_data(sitk_img, dataset)\n",
    "                    nifti_save_path = os.path.join(nifti_root_directory, dicom_folder + '.nii')\n",
    "                    sitk.WriteImage(sitk_img, nifti_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(dataset.pixel_array, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Create Dataset Split (One-Hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import SimpleITK as sitk\n",
    "from sklearn.model_selection import train_test_split, KFold, GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv_dir = '/home/firas/Desktop/work/chest_radiography/data'\n",
    "path_to_csv = '/home/firas/Desktop/work/chest_radiography/data/parsed_chest_xray_p1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV and append 01 to each accession number\n",
    "df = pd.read_csv(path_to_csv)\n",
    "df['Anforderungsnummer'] = df['Anforderungsnummer'].apply(lambda x: str(x) + '01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that have at least one empty cell\n",
    "df_no_nans = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include data rows that are included in the data directory\n",
    "path_to_data_dir = '/data/chest_radiograph/resized_nifti_files'\n",
    "data_files = [file_name.split('.')[0] for file_name in os.listdir(path_to_data_dir)]\n",
    "df_available = df_no_nans[df_no_nans['Anforderungsnummer'].isin(data_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the label columns\n",
    "df_available.columns\n",
    "columns_to_one_hot_encode = df_available.columns[5:]\n",
    "df_one_hot = pd.get_dummies(df_available, columns=columns_to_one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include meta data into the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_hot.insert(5, \"PatientName\", \"\")\n",
    "df_one_hot.insert(6, \"StudyDate\", \"\")\n",
    "df_one_hot.insert(7, \"StudyTime\", \"\")\n",
    "df_one_hot.insert(8, \"PatientSex\", \"\")\n",
    "df_one_hot.insert(9, \"RequestingPhysician\", \"\")\n",
    "df_one_hot.insert(10, \"ExposureinuAs\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = pd.read_csv('/home/firas/Desktop/work/chest_radiography/data/csv_with_names/all_studies.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include meta data into the csv. \n",
    "for accession_number in tqdm(df_one_hot['Anforderungsnummer'].values):\n",
    "    for root, dirs, files in os.walk(os.path.join(dicom_root_directory, accession_number)):\n",
    "        for file in files:\n",
    "                if file.endswith(\".dcm\"):\n",
    "                    dicom_file_name = os.path.join(root, file)\n",
    "                    dataset = pydicom.dcmread(dicom_file_name)\n",
    "                    if dataset.Modality == 'CR':\n",
    "                        try:\n",
    "                            # Write meta data\n",
    "                            index = df_one_hot[df_one_hot['Anforderungsnummer'] == accession_number].index.values[0]\n",
    "                            df_one_hot.at[index, 'PatientName'] = df_names[df_names['Anforderungsnummer'] == accession_number]['Fullname'].values[0]\n",
    "                            df_one_hot.at[index, 'StudyDate'] = str(dataset[0x0008, 0x0020].value)\n",
    "                            df_one_hot.at[index, 'StudyTime'] = str(dataset[0x0008, 0x0030].value)\n",
    "                            df_one_hot.at[index, 'PatientSex'] = str(dataset[0x0010, 0x0040].value)\n",
    "                            df_one_hot.at[index, 'RequestingPhysician'] = str(dataset[0x0032, 0x1032].value)\n",
    "                            df_one_hot.at[index, 'ExposureinuAs'] = str(dataset[0x0018, 0x1153].value)\n",
    "                        except Exception as e:\n",
    "                            if not isinstance(e, KeyError):\n",
    "                                print(e)\n",
    "                            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_hot.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training, testing and validation split and ensure Patient only appears in one set\n",
    "random_state = 379647 # keep the randomization reproducible\n",
    "train_val_inds, test_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state=random_state).split(df_one_hot, groups=df_one_hot['PatientName']))\n",
    "train_val = df_one_hot.iloc[train_val_inds]\n",
    "test = df_one_hot.iloc[test_inds]\n",
    "train_inds, val_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state=random_state).split(train_val, groups=train_val['PatientName']))\n",
    "train = train_val.iloc[train_inds]\n",
    "valid = train_val.iloc[val_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(os.path.join(path_to_csv_dir, 'train.csv'), index=False)\n",
    "valid.to_csv(os.path.join(path_to_csv_dir, 'valid.csv'), index=False)\n",
    "test.to_csv(os.path.join(path_to_csv_dir, 'test.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Split (Custom Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import SimpleITK as sitk\n",
    "from sklearn.model_selection import train_test_split, KFold, GroupShuffleSplit\n",
    "import numpy as np\n",
    "import pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv_dir = '/home/firas/Desktop/work/chest_radiography/data'\n",
    "path_to_csv = '/home/firas/Desktop/work/chest_radiography/data/parsed_chest_xray_p1.csv'\n",
    "dicom_root_directory = '/data/chest_radiograph/dicom_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV and append 01 to each accession number\n",
    "df = pd.read_csv(path_to_csv)\n",
    "df['Anforderungsnummer'] = df['Anforderungsnummer'].apply(lambda x: str(x) + '01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that have at least one empty cell\n",
    "df_no_nans = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include data rows that are included in the data directory\n",
    "path_to_data_dir = '/data/chest_radiograph/resized_nifti_files'\n",
    "data_files = [file_name.split('.')[0] for file_name in os.listdir(path_to_data_dir)]\n",
    "df_available = df_no_nans[df_no_nans['Anforderungsnummer'].isin(data_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_ignore_index = df_available.replace(0.0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with gradual changes\n",
    "for column in ['Stauung', 'Pleuraerguss_re', 'Pleuraerguss_li', 'Infiltrate_re', 'Infiltrate_li', \n",
    "               'Belstörungen_re', 'Belstörungen_li', ]:\n",
    "    df_with_ignore_index[column] = df_with_ignore_index[column].replace({1:0, 5:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with gradual changes for heart\n",
    "is_herz_beurteilbar = df_with_ignore_index['Herzgröße'] != 5.0\n",
    "is_herz_beurteilbar = is_herz_beurteilbar.astype(float)\n",
    "df_with_ignore_index.insert(6, \"is_herzgröße_beurteilbar\", is_herz_beurteilbar)\n",
    "df_with_ignore_index['Herzgröße'] = df_with_ignore_index['Herzgröße'].replace({5:100, 1:0, 2:1, 3:2, 4:3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with location in Belüftungsstörungen \n",
    "\n",
    "# check if Belstörung is present\n",
    "is_belstörung_li_present = (df_with_ignore_index['Belstörungen_li'] != 0).astype(float) # check for 0 because it is a standard label and has been changed above\n",
    "is_belstörung_re_present = (df_with_ignore_index['Belstörungen_re'] != 0).astype(float)\n",
    "\n",
    "# insert the new columns\n",
    "df_with_ignore_index.insert(15, \"is_belstörung_li_present\", is_belstörung_li_present)\n",
    "df_with_ignore_index.insert(13, \"is_belstörung_re_present\", is_belstörung_re_present)\n",
    "\n",
    "\n",
    "# set ignore index where belstörung is not present\n",
    "df_with_ignore_index.loc[df_with_ignore_index['is_belstörung_li_present']==0.0, 'Belstörungenidem_li'] = 100.0\n",
    "\n",
    "# remove the new columns as they are unnecessary for training\n",
    "df_with_ignore_index = df_with_ignore_index.drop(columns=['is_belstörung_li_present', 'is_belstörung_re_present'])\n",
    "\n",
    "# check for OF/UF\n",
    "is_of_uf_li = (df_with_ignore_index['Belstörungenidem_li'] == 6).astype(float)\n",
    "is_of_uf_re = (df_with_ignore_index['Belstörungenidem_re'] == 6).astype(float)\n",
    "\n",
    "# insert the new columns\n",
    "df_with_ignore_index.insert(15, \"Belstörungen_of_uf_li\", is_of_uf_li)\n",
    "df_with_ignore_index.insert(13, \"Belstörungen_of_uf_re\", is_of_uf_re)\n",
    "\n",
    "# set ignore index where OF/UF is present\n",
    "df_with_ignore_index.loc[df_with_ignore_index['Belstörungen_of_uf_li']==1, 'Belstörungenidem_li'] = 100.0\n",
    "df_with_ignore_index.loc[df_with_ignore_index['Belstörungen_of_uf_re']==1, 'Belstörungenidem_re'] = 100.0\n",
    "\n",
    "# Replace the labels\n",
    "df_with_ignore_index['Belstörungen_of_uf_li'] = df_with_ignore_index['Belstörungen_of_uf_li'].replace({6:100, 4:0, 3:1, 7:2, 2:3, 5:4, 1:5})\n",
    "df_with_ignore_index['Belstörungen_of_uf_re'] = df_with_ignore_index['Belstörungen_of_uf_re'].replace({6:100, 4:0, 3:1, 7:2, 2:3, 5:4, 1:5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with Pneumothorax\n",
    "\n",
    "# Check for pneumothorax\n",
    "is_pneumothorax_li_present = ((df_with_ignore_index['Pneumothorax_li'] != 1) & (df_with_ignore_index['Pneumothorax_li'] != 100)).astype(float)\n",
    "is_pneumothorax_re_present = ((df_with_ignore_index['Pneumothorax_re'] != 1) & (df_with_ignore_index['Pneumothorax_re'] != 100)).astype(float)\n",
    "\n",
    "# insert the new columns\n",
    "df_with_ignore_index.insert(19, \"is_pneumothorax_li_present\", is_pneumothorax_li_present)\n",
    "df_with_ignore_index.insert(18, \"is_pneumothorax_re_present\", is_pneumothorax_re_present)\n",
    "\n",
    "# set ignore index where pneumothorax can't be checked \n",
    "df_with_ignore_index.loc[df_with_ignore_index['Pneumothorax_li']==100, 'is_pneumothorax_li_present'] = 100.0\n",
    "df_with_ignore_index.loc[df_with_ignore_index['Pneumothorax_re']==100, 'is_pneumothorax_re_present'] = 100.0\n",
    "\n",
    "# Split the Pneumothorax Labels into an output logit that contains the location labels\n",
    "pneumothorax_li_location = df_with_ignore_index['Pneumothorax_li'].copy()\n",
    "for label in [1, 5, 6, 7]:\n",
    "    pneumothorax_li_location.loc[pneumothorax_li_location==label] = 100.0\n",
    "    \n",
    "pneumothorax_re_location = df_with_ignore_index['Pneumothorax_re'].copy()\n",
    "for label in [1, 5, 6, 7]:\n",
    "    pneumothorax_re_location.loc[pneumothorax_re_location==label] = 100.0\n",
    "\n",
    "# Split the Pneumothorax Labels into an output logit that contains the severity labels\n",
    "pneumothorax_li_severity = df_with_ignore_index['Pneumothorax_li'].copy()\n",
    "for label in [1, 2, 3, 4]:\n",
    "    pneumothorax_li_severity.loc[pneumothorax_li_severity==label] = 100.0\n",
    "\n",
    "pneumothorax_re_severity = df_with_ignore_index['Pneumothorax_re'].copy()\n",
    "for label in [1, 2, 3, 4]:\n",
    "    pneumothorax_re_severity.loc[pneumothorax_re_severity==label] = 100.0\n",
    "    \n",
    "# insert the new columns\n",
    "df_with_ignore_index.insert(22, \"pneumothorax_li_location\", pneumothorax_li_location)\n",
    "df_with_ignore_index.insert(19, \"pneumothorax_re_location\", pneumothorax_re_location)\n",
    "df_with_ignore_index.insert(24, \"pneumothorax_li_severity\", pneumothorax_li_severity)\n",
    "df_with_ignore_index.insert(20, \"pneumothorax_re_severity\", pneumothorax_re_severity)\n",
    "\n",
    "# Drop the original column\n",
    "df_with_ignore_index = df_with_ignore_index.drop(columns=['Pneumothorax_li', 'Pneumothorax_re'])\n",
    "\n",
    "# Relabel the new columns\n",
    "df_with_ignore_index['pneumothorax_li_location'] = df_with_ignore_index['pneumothorax_li_location'].replace({4:0, 3:1, 2:2})\n",
    "df_with_ignore_index['pneumothorax_re_location'] = df_with_ignore_index['pneumothorax_re_location'].replace({4:0, 3:1, 2:2})\n",
    "df_with_ignore_index['pneumothorax_li_severity'] = df_with_ignore_index['pneumothorax_li_severity'].replace({5:0, 6:1, 7:2})\n",
    "df_with_ignore_index['pneumothorax_re_severity'] = df_with_ignore_index['pneumothorax_re_severity'].replace({5:0, 6:1, 7:2})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_ignore_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_hot = df_with_ignore_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include meta data into the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_hot.insert(5, \"PatientName\", \"\")\n",
    "df_one_hot.insert(6, \"StudyDate\", \"\")\n",
    "df_one_hot.insert(7, \"StudyTime\", \"\")\n",
    "df_one_hot.insert(8, \"PatientSex\", \"\")\n",
    "df_one_hot.insert(9, \"RequestingPhysician\", \"\")\n",
    "df_one_hot.insert(10, \"ExposureinuAs\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = pd.read_csv('/home/firas/Desktop/work/chest_radiography/data/csv_with_names/all_studies.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include meta data into the csv. \n",
    "for accession_number in tqdm(df_one_hot['Anforderungsnummer'].values):\n",
    "    for root, dirs, files in os.walk(os.path.join(dicom_root_directory, accession_number)):\n",
    "        for file in files:\n",
    "                if file.endswith(\".dcm\"):\n",
    "                    dicom_file_name = os.path.join(root, file)\n",
    "                    dataset = pydicom.dcmread(dicom_file_name)\n",
    "                    if dataset.Modality == 'CR':\n",
    "                        try:\n",
    "                            # Write meta data\n",
    "                            index = df_one_hot[df_one_hot['Anforderungsnummer'] == accession_number].index.values[0]\n",
    "                            df_one_hot.at[index, 'PatientName'] = df_names[df_names['Anforderungsnummer'] == accession_number]['Fullname'].values[0]\n",
    "                            df_one_hot.at[index, 'StudyDate'] = str(dataset[0x0008, 0x0020].value)\n",
    "                            df_one_hot.at[index, 'StudyTime'] = str(dataset[0x0008, 0x0030].value)\n",
    "                            df_one_hot.at[index, 'PatientSex'] = str(dataset[0x0010, 0x0040].value)\n",
    "                            df_one_hot.at[index, 'RequestingPhysician'] = str(dataset[0x0032, 0x1032].value)\n",
    "                            df_one_hot.at[index, 'ExposureinuAs'] = str(dataset[0x0018, 0x1153].value)\n",
    "                        except Exception as e:\n",
    "                            if not isinstance(e, KeyError):\n",
    "                                print(e)\n",
    "                            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_hot.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training, testing and validation split and ensure Patient only appears in one set\n",
    "random_state = 379647 # keep the randomization reproducible\n",
    "train_val_inds, test_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state=random_state).split(df_one_hot, groups=df_one_hot['PatientName']))\n",
    "train_val = df_one_hot.iloc[train_val_inds]\n",
    "test = df_one_hot.iloc[test_inds]\n",
    "train_inds, val_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state=random_state).split(train_val, groups=train_val['PatientName']))\n",
    "train = train_val.iloc[train_inds]\n",
    "valid = train_val.iloc[val_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(os.path.join(path_to_csv_dir, 'train_custom.csv'), index=False)\n",
    "valid.to_csv(os.path.join(path_to_csv_dir, 'valid_custom.csv'), index=False)\n",
    "test.to_csv(os.path.join(path_to_csv_dir, 'test_custom.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Split (One-Hot and Drop lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv_dir = '/home/firas/Desktop/work/chest_radiography/data'\n",
    "path_to_csv = '/home/firas/Desktop/work/chest_radiography/data/parsed_chest_xray_p1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV and append 01 to each accession number\n",
    "df = pd.read_csv(path_to_csv)\n",
    "df['Anforderungsnummer'] = df['Anforderungsnummer'].apply(lambda x: str(x) + '01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that have at least one empty cell\n",
    "df_no_nans = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include data rows that are included in the data directory\n",
    "path_to_data_dir = '/data/chest_radiograph/resized_nifti_files'\n",
    "data_files = [file_name.split('.')[0] for file_name in os.listdir(path_to_data_dir)]\n",
    "df_available = df_no_nans[df_no_nans['Anforderungsnummer'].isin(data_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 0 with NaN\n",
    "df_available = df_available.replace({0: np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the occurences of Nans in the Dataframe\n",
    "for i in df_available.columns:\n",
    "    print(i, df_available[i].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns that lead to many nans\n",
    "df_available = df_available.drop(columns=['Belstörungenidem_re', 'Belstörungenidem_li'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all nan rows\n",
    "df_available = df_available.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the label columns\n",
    "df_available.columns\n",
    "columns_to_one_hot_encode = df_available.columns[5:]\n",
    "df_one_hot = pd.get_dummies(df_available, columns=columns_to_one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include meta data into the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_hot.insert(5, \"PatientName\", \"\")\n",
    "df_one_hot.insert(6, \"StudyDate\", \"\")\n",
    "df_one_hot.insert(7, \"StudyTime\", \"\")\n",
    "df_one_hot.insert(8, \"PatientSex\", \"\")\n",
    "df_one_hot.insert(9, \"RequestingPhysician\", \"\")\n",
    "df_one_hot.insert(10, \"ExposureinuAs\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = pd.read_csv('/home/firas/Desktop/work/chest_radiography/data/csv_with_names/all_studies.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include meta data into the csv. \n",
    "for accession_number in tqdm(df_one_hot['Anforderungsnummer'].values):\n",
    "    for root, dirs, files in os.walk(os.path.join(dicom_root_directory, accession_number)):\n",
    "        for file in files:\n",
    "                if file.endswith(\".dcm\"):\n",
    "                    dicom_file_name = os.path.join(root, file)\n",
    "                    dataset = pydicom.dcmread(dicom_file_name)\n",
    "                    if dataset.Modality == 'CR':\n",
    "                        try:\n",
    "                            # Write meta data\n",
    "                            index = df_one_hot[df_one_hot['Anforderungsnummer'] == accession_number].index.values[0]\n",
    "                            df_one_hot.at[index, 'PatientName'] = df_names[df_names['Anforderungsnummer'] == accession_number]['Fullname'].values[0]\n",
    "                            df_one_hot.at[index, 'StudyDate'] = str(dataset[0x0008, 0x0020].value)\n",
    "                            df_one_hot.at[index, 'StudyTime'] = str(dataset[0x0008, 0x0030].value)\n",
    "                            df_one_hot.at[index, 'PatientSex'] = str(dataset[0x0010, 0x0040].value)\n",
    "                            df_one_hot.at[index, 'RequestingPhysician'] = str(dataset[0x0032, 0x1032].value)\n",
    "                            df_one_hot.at[index, 'ExposureinuAs'] = str(dataset[0x0018, 0x1153].value)\n",
    "                        except Exception as e:\n",
    "                            if not isinstance(e, KeyError):\n",
    "                                print(e)\n",
    "                            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_hot.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training, testing and validation split and ensure Patient only appears in one set\n",
    "random_state = 379647 # keep the randomization reproducible\n",
    "train_val_inds, test_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state=random_state).split(df_one_hot, groups=df_one_hot['PatientName']))\n",
    "train_val = df_one_hot.iloc[train_val_inds]\n",
    "test = df_one_hot.iloc[test_inds]\n",
    "train_inds, val_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state=random_state).split(train_val, groups=train_val['PatientName']))\n",
    "train = train_val.iloc[train_inds]\n",
    "valid = train_val.iloc[val_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(os.path.join(path_to_csv_dir, 'train.csv'), index=False)\n",
    "valid.to_csv(os.path.join(path_to_csv_dir, 'valid.csv'), index=False)\n",
    "test.to_csv(os.path.join(path_to_csv_dir, 'test.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasplit for Stauung (One-Hot vs Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv_dir = '/home/firas/Desktop/work/chest_radiography/data'\n",
    "path_to_csv = '/home/firas/Desktop/work/chest_radiography/data/parsed_chest_xray_p1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_hot_all = pd.read_csv(os.path.join(path_to_csv_dir, 'train.csv'))\n",
    "valid_one_hot_all = pd.read_csv(os.path.join(path_to_csv_dir, 'valid.csv'))\n",
    "test_one_hot_all = pd.read_csv(os.path.join(path_to_csv_dir, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the columns labels based on severity\n",
    "train_one_hot_all_stauung_ordered = train_one_hot_all.rename(columns={'Staaung_1.0': 'Stauung_1.0', 'Stauung_5.0': 'Stauung_2.0', 'Stauung_2.0': 'Stauung_3.0', 'Stauung_3.0': 'Stauung_4.0', 'Stauung_4.0': 'Stauung_5.0'})\n",
    "valid_one_hot_all_stauung_ordered = valid_one_hot_all.rename(columns={'Staaung_1.0': 'Stauung_1.0', 'Stauung_5.0': 'Stauung_2.0', 'Stauung_2.0': 'Stauung_3.0', 'Stauung_3.0': 'Stauung_4.0', 'Stauung_4.0': 'Stauung_5.0'})\n",
    "test_one_hot_all_stauung_ordered = test_one_hot_all.rename(columns={'Staaung_1.0': 'Stauung_1.0', 'Stauung_5.0': 'Stauung_2.0', 'Stauung_2.0': 'Stauung_3.0', 'Stauung_3.0': 'Stauung_4.0', 'Stauung_4.0': 'Stauung_5.0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all but the Stauung labels\n",
    "train_one_hot_stauung_ordered = train_one_hot_all_stauung_ordered.loc[:, ['Aufnahmenummer', 'Anforderungsnummer', 'Geburtsdatum',\n",
    "       'Untersuchungsdatum', 'Untersuchung Dokumentiert', 'PatientName',\n",
    "       'StudyDate', 'StudyTime', 'PatientSex', 'RequestingPhysician',\n",
    "       'ExposureinuAs','Stauung_1.0', 'Stauung_2.0',\n",
    "       'Stauung_3.0', 'Stauung_4.0', 'Stauung_5.0']]\n",
    "valid_one_hot_stauung_ordered = valid_one_hot_all_stauung_ordered.loc[:, ['Aufnahmenummer', 'Anforderungsnummer', 'Geburtsdatum',\n",
    "       'Untersuchungsdatum', 'Untersuchung Dokumentiert', 'PatientName',\n",
    "       'StudyDate', 'StudyTime', 'PatientSex', 'RequestingPhysician',\n",
    "       'ExposureinuAs','Stauung_1.0', 'Stauung_2.0',\n",
    "       'Stauung_3.0', 'Stauung_4.0', 'Stauung_5.0']]\n",
    "test_one_hot_stauung_ordered = test_one_hot_all_stauung_ordered.loc[:, ['Aufnahmenummer', 'Anforderungsnummer', 'Geburtsdatum',\n",
    "       'Untersuchungsdatum', 'Untersuchung Dokumentiert', 'PatientName',\n",
    "       'StudyDate', 'StudyTime', 'PatientSex', 'RequestingPhysician',\n",
    "       'ExposureinuAs','Stauung_1.0', 'Stauung_2.0',\n",
    "       'Stauung_3.0', 'Stauung_4.0', 'Stauung_5.0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_hot_stauung_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_encoding(row):\n",
    "    label_of_interest = 'Stauung'\n",
    "    \n",
    "    # Get the labels of the series index that match the label of interest\n",
    "    keys = row.keys()\n",
    "    label_keys = [key for key in keys if label_of_interest in key]\n",
    "    \n",
    "    # Get the label name that encodes the 1\n",
    "    values = row[label_keys]\n",
    "    argmax_pos = values.to_numpy().argmax()\n",
    "    argmax_label = values.keys()[argmax_pos]\n",
    "    \n",
    "    # Set all other labels that would have a lower odinal value also to 1\n",
    "    for label_key in label_keys:\n",
    "        row[label_key] = 1 if label_key.split('_')[-1] <= argmax_label.split('_')[-1] else 0\n",
    "    return row\n",
    "\n",
    "train_custom_encoded_stauung = train_one_hot_stauung_ordered.apply(create_custom_encoding, axis=1)\n",
    "valid_custom_encoded_stauung = valid_one_hot_stauung_ordered.apply(create_custom_encoding, axis=1)\n",
    "test_custom_encoded_stauung = test_one_hot_stauung_ordered.apply(create_custom_encoding, axis=1)\n",
    "\n",
    "# Have the Value for None be encoded as 0 0 0 0 (not 1 0 0 0 )\n",
    "train_custom_encoded_stauung = train_custom_encoded_stauung.loc[:, train_custom_encoded_stauung.columns != 'Stauung_1.0']\n",
    "valid_custom_encoded_stauung = valid_custom_encoded_stauung.loc[:, valid_custom_encoded_stauung.columns != 'Stauung_1.0']\n",
    "test_custom_encoded_stauung = test_custom_encoded_stauung.loc[:, test_custom_encoded_stauung.columns != 'Stauung_1.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_hot_stauung_ordered.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_custom_encoded_stauung.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_hot_stauung_ordered.to_csv(os.path.join(path_to_csv_dir, 'train_one_hot_stauung_ordered.csv'), index=False)\n",
    "valid_one_hot_stauung_ordered.to_csv(os.path.join(path_to_csv_dir, 'valid_one_hot_stauung_ordered.csv'), index=False)\n",
    "test_one_hot_stauung_ordered.to_csv(os.path.join(path_to_csv_dir, 'test_one_hot_stauung_ordered.csv'), index=False)\n",
    "\n",
    "train_custom_encoded_stauung.to_csv(os.path.join(path_to_csv_dir, 'train_custom_encoded_stauung.csv'), index=False)\n",
    "valid_custom_encoded_stauung.to_csv(os.path.join(path_to_csv_dir, 'valid_custom_encoded_stauung.csv'), index=False)\n",
    "test_custom_encoded_stauung.to_csv(os.path.join(path_to_csv_dir, 'test_custom_encoded_stauung.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Laboratory Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lab = pd.read_csv(os.path.join(path_to_csv_dir, 'train.csv'))\n",
    "valid_lab = pd.read_csv(os.path.join(path_to_csv_dir, 'valid.csv'))\n",
    "test_lab = pd.read_csv(os.path.join(path_to_csv_dir, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Columns for CRP and BNP\n",
    "train_lab['CRP'] = -1000\n",
    "valid_lab['CRP'] = -1000\n",
    "test_lab['CRP'] = -1000\n",
    "train_lab['BNP'] = -1000\n",
    "valid_lab['BNP'] = -1000\n",
    "test_lab['BNP'] = -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_results_path = '/home/firas/Desktop/work/chest_radiography/data_laborwerte/Radiologie_Truhn_Laborwerte.csv'\n",
    "df_lab_results = pd.read_csv(lab_results_path, engine=\"python\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the date columns from str to datetime\n",
    "train_lab['Untersuchungsdatum'] = pd.to_datetime(train_lab['Untersuchungsdatum'])\n",
    "valid_lab['Untersuchungsdatum'] = pd.to_datetime(valid_lab['Untersuchungsdatum'])\n",
    "test_lab['Untersuchungsdatum'] = pd.to_datetime(test_lab['Untersuchungsdatum'])\n",
    "df_lab_results['Datum'] = pd.to_datetime(df_lab_results['Datum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sign(x):\n",
    "    if str(x).startswith('<'):\n",
    "        return float(x.split('<')[1])\n",
    "    elif str(x).startswith('>'):\n",
    "        return float(x.split('>')[1])\n",
    "    return x\n",
    "\n",
    "df_lab_results['Wert_txt'] = df_lab_results['Wert_txt'].apply(remove_sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_split in [train_lab, valid_lab, test_lab]:\n",
    "    df_lab_results_filtered = df_lab_results[df_lab_results['Aufnahmenummer'].isin(data_split['Aufnahmenummer'])]\n",
    "    df_lab_results_analytics_crp = df_lab_results_filtered[df_lab_results_filtered['Analyt'].isin(['CRP'])]\n",
    "    df_lab_results_analytics_bnp = df_lab_results_filtered[df_lab_results_filtered['Analyt'].isin(['NTpBNP2'])]\n",
    "    for key, lab_values in {'CRP': df_lab_results_analytics_crp, 'BNP': df_lab_results_analytics_bnp}.items():\n",
    "        # Filter out the necessary analytics (CRP and BNP)\n",
    "        for aufnahmenummer in tqdm(lab_values['Aufnahmenummer'].unique()):\n",
    "            lab_results_of_interest = lab_values[lab_values['Aufnahmenummer'] == aufnahmenummer]\n",
    "            for _, scan in data_split[data_split['Aufnahmenummer']==aufnahmenummer].iterrows():\n",
    "                lab_results_after_scan_date = lab_results_of_interest.set_index('Datum').sort_index().loc[scan['Untersuchungsdatum']:]\n",
    "                closest_lab_result_after_scan_date = lab_results_after_scan_date.iloc[0] if not lab_results_after_scan_date.empty else None            \n",
    "                \n",
    "                if (closest_lab_result_after_scan_date is not None) and ((closest_lab_result_after_scan_date.name - scan['Untersuchungsdatum']).days < 2):\n",
    "                    try:\n",
    "                        lab_value = float(closest_lab_result_after_scan_date['Wert_txt'])\n",
    "                        data_split.loc[data_split['Anforderungsnummer'] == scan['Anforderungsnummer'], key] = lab_value\n",
    "                    except ValueError:\n",
    "                        print(\"ValueError was raised\")\n",
    "                        print(closest_lab_result_after_scan_date['Wert_txt'])\n",
    "                        continue\n",
    "\n",
    "# Remove accidental NaNs\n",
    "train_lab.loc[train_lab['CRP'].isna(), 'CRP'] = -1000\n",
    "valid_lab.loc[valid_lab['CRP'].isna(), 'CRP'] = -1000\n",
    "test_lab.loc[test_lab['CRP'].isna(), 'CRP'] = -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_lab['Anforderungsnummer'][test_lab['Anforderungsnummer']==-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_lab_filtered = valid_lab[valid_lab['CRP'] != -1000]\n",
    "test_lab_filtered = test_lab[test_lab['CRP'] != -1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lab.to_csv(os.path.join(path_to_csv_dir, 'train_lab.csv'), index=False)\n",
    "valid_lab_filtered.to_csv(os.path.join(path_to_csv_dir, 'valid_lab.csv'), index=False)\n",
    "test_lab_filtered.to_csv(os.path.join(path_to_csv_dir, 'test_lab.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lab_filtered = train_lab[train_lab['CRP'] != -1000]\n",
    "train_lab_filtered.to_csv(os.path.join(path_to_csv_dir, 'train_lab_filtered.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_lab.to_csv(os.path.join(path_to_csv_dir, 'valid_lab_unfiltered.csv'), index=False)\n",
    "test_lab.to_csv(os.path.join(path_to_csv_dir, 'test_lab_unfiltered.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discretize the CRP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lab_discrete = train_lab.copy()\n",
    "valid_lab_filtered_discrete = valid_lab_filtered.copy()\n",
    "test_lab_filtered_discrete = test_lab_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized_splits = {}\n",
    "for key, split in {'train': train_lab_discrete, 'valid': valid_lab_filtered_discrete, 'test': test_lab_filtered_discrete}.items():\n",
    "    categorized = pd.cut(split.CRP, bins=[0, 70, 140, 210, 280, 999], labels=['+', '++', '+++', '++++', '+++++'])\n",
    "    split['CRP'] = categorized\n",
    "    split_categorized = split.copy()\n",
    "    split = pd.get_dummies(split, columns=['CRP'])\n",
    "    split.loc[split_categorized.CRP.isnull(), split.columns.str.startswith(\"CRP\")] = 99\n",
    "    categorized_splits[key] = split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, split in categorized_splits.items():\n",
    "    split = split.drop(columns=['BNP']) \n",
    "    split.to_csv(os.path.join(path_to_csv_dir, f'{key}_lab_categorized_CRP'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_lab_filtered = valid_lab[valid_lab['CRP'] != -1000]\n",
    "test_lab_filtered = test_lab[test_lab['CRP'] != -1000]\n",
    "train_lab_filtered = train_lab[train_lab['CRP'] != -1000]\n",
    "\n",
    "len(train_lab_filtered['PatientName'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_lab_filtered[train_lab_filtered['CRP']<5]['PatientName'].unique()))\n",
    "print(len(train_lab_filtered[(train_lab_filtered['CRP']>=5) & (train_lab_filtered['CRP']<=50)]['PatientName'].unique()))\n",
    "print(len(train_lab_filtered[train_lab_filtered['CRP']>50]['PatientName'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lab_filtered['CRP'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
